{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-convolutions-in-action.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyONxpWNCHMWhG//gmMKRmUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bde14bd4bdc1455da995c752d84fefbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_47bdab32c6194c1ea8928d701c266a96",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72cc2904ae3e4d0aa8615c886524af00",
              "IPY_MODEL_732aae429d084a42898b7048543ee342"
            ]
          }
        },
        "47bdab32c6194c1ea8928d701c266a96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72cc2904ae3e4d0aa8615c886524af00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_230fc2e557ea40518f8e6c028a45ef88",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a225564af0549e790e2f291ea7c5478"
          }
        },
        "732aae429d084a42898b7048543ee342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_707f10128e7c4ac583eddb8fe2948c38",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:09&lt;00:00, 18807453.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5599fea36f8a4935b1c58d809467d956"
          }
        },
        "230fc2e557ea40518f8e6c028a45ef88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a225564af0549e790e2f291ea7c5478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "707f10128e7c4ac583eddb8fe2948c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5599fea36f8a4935b1c58d809467d956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-with-pytorch/blob/master/8-using-convolutions-to-generalize/1_convolutions_in_action.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD-pGnbRyH6s"
      },
      "source": [
        "# Using convolutions to generalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QS-_bhIydI7"
      },
      "source": [
        "Due to the fully connected setup needed to detect the various possible translations of the bird or airplane in the image, we have both too many parameters (making it easier for the model to memorize the training set) and no position independence (making it harder to generalize). \r\n",
        "\r\n",
        "As we know, we could augment our training data by using a wide variety of recropped images to try to force generalization, but that won’t address the issue of having too many parameters.\r\n",
        "**There is a better way! It consists of replacing the dense, fully connected affine transformation in our neural network unit with a different linear operation: convolution.**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRKStm2QzDAv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fRJTzpxzEj7"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import collections\r\n",
        "import datetime\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "torch.set_printoptions(edgeitems=2)\r\n",
        "torch.manual_seed(123)\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "bde14bd4bdc1455da995c752d84fefbb",
            "47bdab32c6194c1ea8928d701c266a96",
            "72cc2904ae3e4d0aa8615c886524af00",
            "732aae429d084a42898b7048543ee342",
            "230fc2e557ea40518f8e6c028a45ef88",
            "4a225564af0549e790e2f291ea7c5478",
            "707f10128e7c4ac583eddb8fe2948c38",
            "5599fea36f8a4935b1c58d809467d956"
          ]
        },
        "id": "-T4StfdbzKPO",
        "outputId": "28bad804-8804-4215-a621-9739d4aaacab"
      },
      "source": [
        "data_path = '../data-unversioned/p1ch6/'\r\n",
        "cifar10 = datasets.CIFAR10(\r\n",
        "    data_path, train=True, download=True,\r\n",
        "    transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\r\n",
        "                             (0.2470, 0.2435, 0.2616))\r\n",
        "    ]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bde14bd4bdc1455da995c752d84fefbb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zOHe4ySzqB1",
        "outputId": "319d5f2e-72b7-4299-fe3d-21342f37ff3b"
      },
      "source": [
        "cifar10_val = datasets.CIFAR10(\r\n",
        "    data_path, train=False, download=True,\r\n",
        "    transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\r\n",
        "                             (0.2470, 0.2435, 0.2616))\r\n",
        "    ]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASLMMCKzztlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f491bdf0-d49c-41dc-f5a3-2b117e5ed2d5"
      },
      "source": [
        "label_map = {0: 0, 2: 1}\r\n",
        "class_names = ['airplane', 'bird']\r\n",
        "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\r\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfKQQ6X00TyU"
      },
      "source": [
        "## The case for convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-79ivp0UzG"
      },
      "source": [
        "We know that taking a 1D view of our input image and multiplying it by an\r\n",
        "n_output_features × n_input_features weight matrix, as is done in nn.Linear,\r\n",
        "means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn5xF0Uhz2es"
      },
      "source": [
        "connected_model = nn.Sequential(\r\n",
        "            nn.Linear(3072, 1024),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(1024, 512),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(512, 128),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(128, 2))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1F_nv30kBn",
        "outputId": "e35a236c-74d1-4fa1-e3b5-2076ff86adc1"
      },
      "source": [
        "numel_list = [p.numel()\r\n",
        "              for p in connected_model.parameters()\r\n",
        "              if p.requires_grad == True]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfUq-5Wg1Pyk"
      },
      "source": [
        "first_model = nn.Sequential(\r\n",
        "                nn.Linear(3072, 512),\r\n",
        "                nn.Tanh(),\r\n",
        "                nn.Linear(512, 2),\r\n",
        "                nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ejlQD701TAj",
        "outputId": "9b0c2b4e-7a0c-4fd8-d4d6-1d0cdb9836e6"
      },
      "source": [
        "numel_list = [p.numel() for p in first_model.parameters()]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1574402, [1572864, 512, 1024, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWhqZSQG1XOY",
        "outputId": "7073a2b0-316f-4c78-95b2-57be90dd6583"
      },
      "source": [
        "linear = nn.Linear(3072, 1024)\r\n",
        "\r\n",
        "linear.weight.shape, linear.bias.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 3072]), torch.Size([1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSstZM7603UJ"
      },
      "source": [
        "In order to translate this intuition into mathematical form, we could compute the weighted sum of a pixel with its immediate neighbors, rather than with all other pixels in the image. This would be equivalent to building weight matrices, one per output feature and output pixel location, in which all weights beyond a certain distance from a center pixel are zero. This will still be a weighted sum: that is, a linear operation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQENTdNY1a74"
      },
      "source": [
        "## What convolutions do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvVc6eWe1eZB"
      },
      "source": [
        "We identified one more desired property earlier: we would like these localized patterns to have an effect on the output regardless of their location in the image: that is, to be translation invariant.\r\n",
        "\r\n",
        "**Fortunately, there is a readily available, local, translation-invariant linear operation on the image: a convolution.**\r\n",
        "\r\n",
        "**Convolution, or more precisely, discrete convolution is defined for a 2D image as the scalar product of a weight matrix, the kernel, with every neighborhood in the input.**\r\n",
        "\r\n",
        "Consider a 3 × 3 kernel (in deep learning, we typically use small kernels; we’ll see why later on) as a 2D tensor.\r\n",
        "\r\n",
        "```python\r\n",
        "weight = torch.tensor([\r\n",
        "  [w00, w01, w02],\r\n",
        "  [w10, w11, w12],\r\n",
        "  [w20, w21, w22]\r\n",
        "])\r\n",
        "```\r\n",
        "\r\n",
        "and a 1-channel, MxN image:\r\n",
        "\r\n",
        "```python\r\n",
        "image = torch.tensor([\r\n",
        "  [i00, i01, i02, i03, ..., i0N],\r\n",
        "  [i10, i11, i12, i13, ..., i1N],\r\n",
        "  [i20, i21, i22, i23, ..., i2N],\r\n",
        "  [i30, i31, i32, i33, ..., i3N],\r\n",
        "  ...\r\n",
        "  [iM0, iM1m iM2, iM3, ..., iMN]\r\n",
        "])\r\n",
        "```\r\n",
        "\r\n",
        "We can compute an element of the output image (without bias) as follows:\r\n",
        "\r\n",
        "```python\r\n",
        "o11 = i11 * w00 + i12 * w01 + i22 * w02 +\r\n",
        "      i21 * w10 + i22 * w11 + i23 * w12 +\r\n",
        "      i31 * w20 + i32 * w21 + i33 * w22\r\n",
        "```\r\n",
        "\r\n",
        "That is, we “translate” the kernel on the i11 location of the input image, and we multiply each weight by the value of the input image at the corresponding location. Thus, the output image is created by translating the kernel on all input locations and performing the weighted sum. For a multichannel image, like our RGB image, the weight matrix would be a 3 × 3 × 3 matrix: one set of weights for every channel, contributing together to the output values.\r\n",
        "\r\n",
        "Note that, just like the elements in the weight matrix of nn.Linear, the weights in the kernel are not known in advance, but they are initialized randomly and updated through backpropagation. Note also that the same kernel, and thus each weight in the kernel, is reused across the whole image. Thinking back to autograd, this means the use of each weight has a history spanning the entire image. Thus, the derivative of the loss with respect to a convolution weight includes contributions from the entire image.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/locality-and-translation-invariance.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "It’s now possible to see the connection to what we were stating earlier:**a convolution is equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.**\r\n",
        "\r\n",
        "Summarizing, by switching to convolutions, we get:\r\n",
        "\r\n",
        "- **Local operations on neighborhoods**\r\n",
        "- **Local operations on neighborhoods**\r\n",
        "- **Models with a lot fewer parameters**\r\n",
        "\r\n",
        "The key insight underlying the third point is that, with a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case in our fully connected model, but rather on the size of the convolution kernel (3 × 3, 5 × 5, and so on) and on how many convolution filters (or output channels) we decide to use in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmHH9aRu6gW1"
      },
      "source": [
        "## Convolutions in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSjCHkTU6jMf"
      },
      "source": [
        "Let’s see some PyTorch in action on our birds versus airplanes challenge. The torch.nn module provides convolutions for 1, 2, and 3 dimensions: nn.Conv1d for time series, nn.Conv2d for images, and nn.Conv3d for volumes or videos.\r\n",
        "\r\n",
        "At a minimum, the arguments we provide to nn.Conv2d are the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel), the number of output features, and the size of the kernel.\r\n",
        "\r\n",
        "The more channels in the output image, the more the capacity of the network. We need the channels to be able to detect many different types of features.\r\n",
        "\r\n",
        "It is very common to have kernel sizes that are the same in all directions, so\r\n",
        "PyTorch has a shortcut for this: whenever kernel_size=3 is specified for a 2D convolution, it means 3 × 3 (provided as a tuple (3, 3) in Python). For a 3D convolution, it means 3 × 3 × 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HsvvkNm0m2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198b1dbf-7308-4b54-8567-a2f8511762e3"
      },
      "source": [
        "conv = nn.Conv2d(3, 16, kernel_size=3)\r\n",
        "conv"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzIhOZswVDjd"
      },
      "source": [
        "What do we expect to be the shape of the weight tensor? \r\n",
        "\r\n",
        "The kernel is of size `3 × 3`, so we want the weight to consist of `3 × 3` parts. For a single output pixel value, our kernel would consider, say, `in_ch = 3` input channels, so the weight component for a single output pixel value (and by translation the invariance for the entire output channel) is of shape `in_ch × 3 × 3`. Finally, we have as many of those as we have output channels, here `out_ch = 16`, so the complete weight tensor is `out_ch × in_ch × 3 × 3`, in our case `16 × 3 × 3 × 3`. The bias will have size 16.\r\n",
        "\r\n",
        "Let’s verify our assumptions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M61bQt5pUVL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a51b2f4-a51d-4d3e-e0b7-8ea8f78a97ae"
      },
      "source": [
        "conv.weight.shape, conv.bias.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_wQGVFvVoLK"
      },
      "source": [
        "We can see how convolutions are a convenient choice for learning from images. We\r\n",
        "have smaller models looking for local patterns whose weights are optimized across the entire image.\r\n",
        "\r\n",
        "A 2D convolution pass produces a 2D image as output, whose pixels are a weighted\r\n",
        "sum over neighborhoods of the input image.\r\n",
        "\r\n",
        "As usual, we need to add the zeroth batch dimension with\r\n",
        "unsqueeze if we want to call the conv module with one input image, since nn.Conv2d expects a B × C × H × W shaped tensor as input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_djc2dVVc0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df20f749-78d1-4192-bda7-fc223c6cf83a"
      },
      "source": [
        "img, _ = cifar2[0]\r\n",
        "output = conv(img.unsqueeze(0))\r\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceth4g6tWUqW"
      },
      "source": [
        "We’re curious, so we can display the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJEcnoEWPnI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8f5d0ad9-7ab6-469d-d2d4-1e5aef3364c6"
      },
      "source": [
        "plt.imshow(output[0, 0].detach(), cmap=\"gray\")\r\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9evxvQbWifv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3039a6cc-750c-4fdb-c7aa-6e29ee881893"
      },
      "source": [
        "plt.figure(figsize=(10, 4.8)) \r\n",
        "ax1 = plt.subplot(1, 2, 1)   \r\n",
        "plt.title('output')   \r\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\r\n",
        "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1) \r\n",
        "plt.imshow(img.mean(0), cmap='gray')  \r\n",
        "plt.title('input')  \r\n",
        " \r\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x345.6 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WDcxQIAXAN3"
      },
      "source": [
        "Wait a minute. Let’s take a look a the size of output: it’s `torch.Size([1, 16, 30, 30])`. Huh; we lost a few pixels in the process. How did that happen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4aamEQvXGzK"
      },
      "source": [
        "## Padding the boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPStthf9XH13"
      },
      "source": [
        "By default, PyTorch will slide the convolution kernel within the input picture, getting `width - kernel_width + 1` horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, 3//2 = 1) smaller on each side. This explains why we’re missing two pixels in each dimension.\r\n",
        "\r\n",
        "However, PyTorch gives us the possibility of padding the image by creating ghost pixels around the border that have value zero as far as the convolution is concerned.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/zero-padding.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "In our case, specifying padding=1 when kernel_size=3 means i00 has an extra set\r\n",
        "of neighbors above it and to its left, so that an output of the convolution can be computed even in the corner of our original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3VzAsO4Wyuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753718b7-edac-4ec5-cc48-5e78a7df362a"
      },
      "source": [
        "# Now with padding\r\n",
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\r\n",
        "output = conv(img.unsqueeze(0))\r\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUxNnVYc33A"
      },
      "source": [
        "There are two main reasons to pad convolutions. \r\n",
        "\r\n",
        "First, doing so helps us separate the matters of convolution and changing image sizes, so we have one less thing to remember. \r\n",
        "\r\n",
        "And second, when we have more elaborate structures such as skip connections or the U-Nets we’ll cover in part 2, we want the tensors before and after a few convolutions to be of compatible size so that we can add them or take differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8GFtKsIdDM4"
      },
      "source": [
        "## Detecting features with convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-V1r3JtdGQK"
      },
      "source": [
        "Weight and bias are parameters that are learned through backpropagation,\r\n",
        "exactly as it happens for weight and bias in nn.Linear. However, we can\r\n",
        "play with convolution by setting weights by hand and see what happens.\r\n",
        "\r\n",
        "Let’s first zero out bias, just to remove any confounding factors, and then set\r\n",
        "weights to a constant value so that each pixel in the output gets the mean of its neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR6D0sRScSuZ"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "  conv.bias.zero_()\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  conv.weight.fill_(1.0 / 9.0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKzL-y797dKQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a98bb84d-ec2a-4b64-afb0-993d614f2428"
      },
      "source": [
        "output = conv(img.unsqueeze(0))\r\n",
        "plt.figure(figsize=(10, 4.8))  # bookskip\r\n",
        "ax1 = plt.subplot(1, 2, 1)   # bookskip\r\n",
        "plt.title('output')   # bookskip\r\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\r\n",
        "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  # bookskip\r\n",
        "plt.imshow(img.mean(0), cmap='gray')  # bookskip\r\n",
        "plt.title('input')  # bookskip\r\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x345.6 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9IkQxHzAZYb"
      },
      "source": [
        "As we could have predicted, the filter produces a blurred version of the image.After all, every pixel of the output is the average of a neighborhood of the\r\n",
        "input, so pixels in the output are correlated and change more smoothly.\r\n",
        "\r\n",
        "Next, let’s try something different. The following kernel may look a bit mysterious at first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joghodwEAMZ9"
      },
      "source": [
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  conv.weight[:] = torch.tensor([\r\n",
        "    [-1.0, 0.0, 1.0],\r\n",
        "    [-1.0, 0.0, 1.0],\r\n",
        "    [-1.0, 0.0, 1.0]                             \r\n",
        "  ])\r\n",
        "  conv.bias.zero_()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBE6SxwTBBSI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7b7df1b0-cc57-4fbd-bd16-50ad530d00a0"
      },
      "source": [
        "output = conv(img.unsqueeze(0))\r\n",
        "plt.figure(figsize=(10, 4.8))  # bookskip\r\n",
        "ax1 = plt.subplot(1, 2, 1)   # bookskip\r\n",
        "plt.title('output')   # bookskip\r\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\r\n",
        "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  # bookskip\r\n",
        "plt.imshow(img.mean(0), cmap='gray')  # bookskip\r\n",
        "plt.title('input')  # bookskip\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x345.6 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhiB1FKRF4v7"
      },
      "source": [
        "Working out the weighted sum for an arbitrary pixel in position 2,2, as we did earlier for the generic convolution kernel, we get\r\n",
        "\r\n",
        "```python\r\n",
        "o22 = i13 - i11 +\r\n",
        "      i23 - i21 +\r\n",
        "      i33 - i31\r\n",
        "```\r\n",
        "\r\n",
        "which performs the difference of all pixels on the right of i22 minus the pixels on the left of i22. If the kernel is applied on a vertical boundary between two adjacent regions of different intensity, o22 will have a high value. If the kernel is applied on a region of uniform intensity, o22 will be zero. \r\n",
        "\r\n",
        "**It’s an edge-detection kernel: the kernel highlights the vertical edge between two horizontally adjacent regions.**\r\n",
        "\r\n",
        "With deep learning, we let kernels be estimated from data in whatever way the discrimination is most effective: for instance, in terms of minimizing the negative crossentropy loss between the output and the ground truth.\r\n",
        "\r\n",
        "From this angle, the job of a convolutional neural network is to estimate the kernel of a set of filter banks in successive layers that will transform a multichannel image into another multichannel image, where different channels correspond to different features (such as one channel for the average, another channel for vertical edges, and so on).\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/convolutions-learning-process.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI2abTOXIZhB"
      },
      "source": [
        "## Looking further with depth and pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkZLWpCbIajG"
      },
      "source": [
        "This is all well and good, but conceptually there’s an elephant in the room. We got all excited because by moving from fully connected layers to convolutions, we achieve locality and translation invariance. Then we recommended the use of small kernels, like 3 × 3, or 5 × 5: that’s peak locality, all right.\r\n",
        "\r\n",
        "**FROM LARGE TO SMALL: DOWNSAMPLING**\r\n",
        "\r\n",
        "Downsampling could in principle occur in different ways. Scaling an image by half is the equivalent of taking four neighboring pixels as input and producing one pixel as output. How we compute the value of the output based on the values of the input is up to us. We could\r\n",
        "\r\n",
        "- **Average the four pixels**. This average pooling was a common approach early on but has fallen out of favor somewhat.\r\n",
        "- **Take the maximum of the four pixels**. This approach, called max pooling, is currently the most commonly used approach, but it has a downside of discarding the other three-quarters of the data.\r\n",
        "- **Perform a strided convolution, where only every Nth pixel is calculated**. A 3 × 4 convolution with stride 2 still incorporates input from all pixels from the previous layer. The literature shows promise for this approach, but it has not yet supplanted max pooling.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/max-pooling.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "By keeping the highest value in the 2 × 2 neighborhood as the downsampled\r\n",
        "output, we ensure that the features that are found survive the downsampling,\r\n",
        "at the expense of the weaker responses.\r\n",
        "\r\n",
        "Max pooling is provided by the nn.MaxPool2d module (as with convolution, there are versions for 1D and 3D data). It takes as input the size of the neighborhood over which to operate the pooling operation. If we wish to downsample our image by half, we’ll want to use a size of 2. \r\n",
        "\r\n",
        "Let’s verify that it works as expected directly on our input image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ5XDwRVvSSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb0aeb4-09c9-4ba7-caf2-2311547ec02d"
      },
      "source": [
        "pool = nn.MaxPool2d(2)\r\n",
        "output = pool(img.unsqueeze(0))\r\n",
        "\r\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB6klnpatya8"
      },
      "source": [
        "**COMBINING CONVOLUTIONS AND DOWNSAMPLING FOR GREAT GOOD**\r\n",
        "\r\n",
        "Let’s now see how combining convolutions and downsampling can help us recognize\r\n",
        "larger structures. we start by applying a set of 3 × 3 kernels on our 8 × 8\r\n",
        "image, obtaining a multichannel output image of the same size. Then we scale down the output image by half, obtaining a 4 × 4 image, and apply another set of 3 × 3 kernels to it. This second set of kernels operates on a 3 × 3 neighborhood of something that has been scaled down by half, so it effectively maps back to 8 × 8 neighborhoods of the input. In addition, the second set of kernels takes the output of the first set of kernels (features like averages, edges, and so on) and extracts additional features on top of those.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/max-pooling-ops.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "So, on one hand, the first set of kernels operates on small neighborhoods on firstorder, low-level features, while the second set of kernels effectively operates on wider neighborhoods, producing features that are compositions of the previous features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiItKYOGw3D6"
      },
      "source": [
        "## Putting it all together for our network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1edjoyfl1iLE"
      },
      "source": [
        "With these building blocks in our hands, we can now proceed to build our convolutional neural network for detecting birds and airplanes.\r\n",
        "\r\n",
        "The first convolution takes us from 3 RGB channels to 16, thereby giving the network a chance to generate 16 independent features that operate to (hopefully) discriminate low-level features of birds and airplanes. Then we apply the Tanh activation function. The resulting 16-channel 32 × 32 image is pooled to a 16-channel 16 × 16 image by the first MaxPool3d. At this point, the downsampled image undergoes another convolution that generates an 8-channel 16 × 16 output. With any luck, this output will consist of higher-level features. Again, we apply a Tanh activation and then pool to an 8-channel 8 × 8 output.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/conv-arch.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Thinking back to the beginning of this chapter, we already know what we need to\r\n",
        "do: turn the 8-channel 8 × 8 image into a 1D vector and complete our network with a set of fully connected layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YRIRum5BKj2"
      },
      "source": [
        "model = nn.Sequential(\r\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\r\n",
        "    nn.Tanh(),\r\n",
        "    nn.MaxPool2d(2),\r\n",
        "\r\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\r\n",
        "    nn.Tanh(),\r\n",
        "    nn.MaxPool2d(2),\r\n",
        "\r\n",
        "    # ...<1>\r\n",
        "\r\n",
        "    nn.Linear(8 * 8 * 8, 32),\r\n",
        "    nn.Tanh(),\r\n",
        "    nn.Linear(32, 2)\r\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixhmK1J_27MS"
      },
      "source": [
        "Ignore the “something missing” comment for a minute. Let’s first notice that the size of the linear layer is dependent on the expected size of the output of MaxPool2d: 8 × 8 × 8 = 512. Let’s count the number of parameters for this small model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idEBdwDi20SM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1ef310-eee6-4503-aa03-cfbdd378ae66"
      },
      "source": [
        "numel_list = [p.numel() for p in model.parameters()]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xthpjWPl3sfc"
      },
      "source": [
        "We put the “Warning” note in the code for a reason. The model has zero chance of\r\n",
        "running without complaining:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs8ISdOv3mVF"
      },
      "source": [
        "# model(img.unsqueeze(0))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvvYhkD04B2t"
      },
      "source": [
        "Admittedly, the error message is a bit obscure, but not too much so. We find references to linear in the traceback: looking back at the model, we see that only module that has to have a 512 × 32 tensor is nn.Linear(512, 32), the first linear module after the last convolution block.\r\n",
        "\r\n",
        "What’s missing there is the reshaping step from an 8-channel 8 × 8 image to a 512-element, 1D vector (1D if we ignore the batch dimension, that is). This could be achieved by calling view on the output of the last nn.MaxPool2d, but unfortunately, we don’t have any explicit visibility of the output of each module when we use nn.Sequential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ANOLxcEYQk"
      },
      "source": [
        "## Subclassing nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEgJANh1revX"
      },
      "source": [
        "When we want to build models that do more complex things than just applying\r\n",
        "one layer after another, we need to leave for something that gives us\r\n",
        "nn.Sequential added flexibility. PyTorch allows us to use any computation in our model by subclassing nn.Module.\r\n",
        "\r\n",
        "In order to subclass nn.Module, at a minimum we need to define a function\r\n",
        "forward that takes the inputs to the module and returns the output. This is where we define our module’s computation.\r\n",
        "\r\n",
        "With PyTorch, if we use standard torch operations, autograd will take care of the backward pass automatically; and indeed, an nn.Module never comes with a backward.\r\n",
        "\r\n",
        "Typically, our computation will use other modules—premade like convolutions or\r\n",
        "customized. To include these submodules, we typically define them in the constructor `__init__` and assign them to self for use in the forward function. They will, at the same time, hold their parameters throughout the lifetime of our module. Note that you need to call `super().__init__()` before you can do that (or PyTorch will remind you)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxjdvnWSsOsX"
      },
      "source": [
        "### Our network as an nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQLkX_KsSw-"
      },
      "source": [
        "Let’s write our network as a submodule. To do so, we instantiate all the nn.Conv2d, nn.Linear, and so on that we previously passed to nn.Sequential in the constructor, and then use their instances one after another in forward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-ooeLPo3xUi"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\r\n",
        "    self.activation1 = nn.Tanh()\r\n",
        "    self.pool1 = nn.MaxPool2d(2)\r\n",
        "\r\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\r\n",
        "    self.activation2 = nn.Tanh()\r\n",
        "    self.pool2 = nn.MaxPool2d(2)\r\n",
        "\r\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\r\n",
        "    self.activation3 = nn.Tanh()\r\n",
        "    self.fc2 = nn.Linear(32, 2)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = self.pool1(self.activation1(self.conv1(x)))\r\n",
        "    out = self.pool2(self.activation2(self.conv2(out)))\r\n",
        "    # This reshape is what we were missing earlier.\r\n",
        "    out = out.view(-1, 8 * 8 * 8)\r\n",
        "    out = self.activation3(self.fc1(out))\r\n",
        "    out = self.fc2(out)\r\n",
        "\r\n",
        "    return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3pUUlnY0i_K"
      },
      "source": [
        "Note that we leave the batch dimension as –1 in the call to view, since in principle we don’t know how many samples will be in the batch.\r\n",
        "\r\n",
        "Recall that the goal of classification networks typically is to compress information in the sense that we start with an image with a sizable number of pixels and compress it into (a vector of probabilities of) classes. Two things about our architecture deserve some commentary with respect to this goal.\r\n",
        "\r\n",
        "First, our goal is reflected by the size of our intermediate values generally\r\n",
        "shrinking—this is done by reducing the number of channels in the convolutions, by reducing the number of pixels through pooling, and by having an output dimension lower than the input dimension in the linear layers. This is a common trait of classification networks. However, the reduction is achieved by pooling in the spatial resolution, but the number of channels increases (still resulting in a reduction in size). It seems that our pattern of fast information reduction works well with networks of limited depth and small images; but for deeper networks, the decrease is typically slower.\r\n",
        "\r\n",
        "Second, in one layer, there is not a reduction of output size with regard to input size: the initial convolution. If we consider a single output pixel as a vector of 32 elements (the channels), it is a linear transformation of 27 elements (as a convolution of 3 channels × 3 × 3 kernel size)—only a moderate increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TfnY4Kn1XyG"
      },
      "source": [
        "### How PyTorch keeps track of parameters and submodules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL3G1KZ01Zhe"
      },
      "source": [
        "Interestingly, assigning an instance of nn.Module to an attribute in an nn.Module, automatically registers the module as a submodule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdDoN09i0YoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d54d15b-5875-4144-e486-7fa822538a1b"
      },
      "source": [
        "net = Net()\r\n",
        "\r\n",
        "numel_list = [p.numel() for p in model.parameters()]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uclVkuGA4jkN"
      },
      "source": [
        "What happens here is that the call delves into all submodules assigned\r\n",
        "parameters() as attributes in the constructor and recursively calls\r\n",
        "on them. No matter parameters()  how nested the submodule, any can access the list of all child parameters. nn.Module By accessing their grad attribute, which has been populated by autograd, the optimizer will know how to change parameters to minimize the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDymMIAEFNd2"
      },
      "source": [
        "### The functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE8o18vkFOq3"
      },
      "source": [
        "By “functional” here we mean “having no internal state”—in other words, “whose output value is solely and fully determined by the value input arguments.”\r\n",
        "\r\n",
        "Indeed, torch.nn.functional provides many functions that work like the modules we find in nn. But instead of working on the input arguments and stored parameters like the module counterparts, they take inputs and parameters as arguments to the function call.\r\n",
        "\r\n",
        "For instance, the functional counterpart of nn.Linear is nn.functional.linear,\r\n",
        "which is a function that has signature linear(input, weight, bias=None). The\r\n",
        "weight and bias parameters are arguments to the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al3gxV1K4P76"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\r\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\r\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\r\n",
        "    self.fc2 = nn.Linear(32, 2)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\r\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\r\n",
        "    out = out.view(-1, 8 * 8 * 8)\r\n",
        "    out = torch.tanh(self.fc1(out))\r\n",
        "    out = self.fc2(out)\r\n",
        "\r\n",
        "    return out"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFpo5Jh0Ijr7"
      },
      "source": [
        "Thus, the functional way also sheds light on what the nn.Module API is all about: a Module is a container for state in the forms of Parameters and submodules combined with the instructions to do a forward.\r\n",
        "\r\n",
        "Whether to use the functional or the modular API is a decision based on style and taste. When part of a network is so simple that we want to use nn.Sequential, we’re in the modular realm. When we are writing our own forwards, it may be more natural to use the functional interface for things that do not need state in the form of parameters.\r\n",
        "\r\n",
        "So now we can make our own nn.Module if we need to, and we also have the functional API for cases when instantiating and then calling an nn.Module is overkill.\r\n",
        "\r\n",
        "Let’s double-check that our model runs, and then we’ll get to the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmwhFIGeIXrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40dfc0c6-4c86-4df1-9667-d59536b95f87"
      },
      "source": [
        "model = Net()\r\n",
        "model(img.unsqueeze(0))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1720, 0.1507]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyU4XAGPJpc7"
      },
      "source": [
        "We got two numbers! Information flows correctly. We might not realize it right now, but in more complex models, getting the size of the first linear layer right is sometimes a source of frustration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beTvODnmJrsp"
      },
      "source": [
        "## Training our convnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5nAGUC3JviO"
      },
      "source": [
        "We’re now at the point where we can assemble our complete training loop.\r\n",
        "\r\n",
        "The core of our convnet is two nested loops: an outer one over the epochs and an inner one of the DataLoader that produces batches from our Dataset. \r\n",
        "\r\n",
        "In each loop, we then have to\r\n",
        "\r\n",
        "1. Feed the inputs through the model (the forward pass).\r\n",
        "2. Compute the loss (also part of the forward pass).\r\n",
        "3. Zero any old gradients.\r\n",
        "4. Call loss.backward() to compute the gradients of the loss with respect to all\r\n",
        "parameters (the backward pass).\r\n",
        "5. Have the optimizer take a step in toward lower loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsbVA_WeJSyM"
      },
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\r\n",
        "  # loop over the epochs, numbered from 1 to n_epochs rather than starting at 0\r\n",
        "  for epoch in range(1, n_epochs + 1):\r\n",
        "    loss_train = 0.0\r\n",
        "    # Loops over dataset in the batches the data loader creates for us\r\n",
        "    for imgs, labels in train_loader:\r\n",
        "      # Feeds a batch through our model\r\n",
        "      outputs = model(imgs)\r\n",
        "      # and computes the loss we wish to minimize\r\n",
        "      loss = loss_fn(outputs, labels)\r\n",
        "      # After getting rid of the gradients from the last round\r\n",
        "      optimizer.zero_grad()\r\n",
        "      # performs the backward step. That is, we compute the gradients of all parameters we want the network to learn.\r\n",
        "      loss.backward()\r\n",
        "      # Updates the model\r\n",
        "      optimizer.step()\r\n",
        "      # Sums the losses\r\n",
        "      loss_train += loss.item()\r\n",
        "    if epoch == 1 or epoch % 10 == 0:\r\n",
        "      # Divides by the length of the training data loader to get the average loss per batch.\r\n",
        "      print(\"{} Epoch {}, Training loss {}\".format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeQ8xgBkSi6g"
      },
      "source": [
        "We use the Dataset; wrap it into a DataLoader; instantiate our network,\r\n",
        "an optimizer, and a loss function as before; and call our training loop.\r\n",
        "\r\n",
        "Let’s run training for 100 epochs while printing the loss. Depending on your hardware, this may take 20 minutes or more to finish!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpC7cfe_Sg79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c835d6-330c-449d-de19-1112191bb0c0"
      },
      "source": [
        "# The DataLoader batches up the examples of our cifar2 dataset. \r\n",
        "# Shuffling randomizes the order of the examples from the dataset.\r\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\r\n",
        "\r\n",
        "# Instantiates our network …\r\n",
        "model = Net()\r\n",
        "\r\n",
        "# the stochastic gradient descent optimizer we have been working with\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\r\n",
        "# and the cross entropy loss\r\n",
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "training_loop(n_epochs=100, optimizer=optimizer, model=model, loss_fn=loss_fn, train_loader=train_loader)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-18 06:44:29.795152 Epoch 1, Training loss 0.5786836488991026\n",
            "2021-01-18 06:45:00.125941 Epoch 10, Training loss 0.3248441396815002\n",
            "2021-01-18 06:45:33.411864 Epoch 20, Training loss 0.2937185798481012\n",
            "2021-01-18 06:46:06.318092 Epoch 30, Training loss 0.2710140582862174\n",
            "2021-01-18 06:46:39.502667 Epoch 40, Training loss 0.2544134628905612\n",
            "2021-01-18 06:47:12.510468 Epoch 50, Training loss 0.24225453652773693\n",
            "2021-01-18 06:47:45.723106 Epoch 60, Training loss 0.2242267387118309\n",
            "2021-01-18 06:48:18.840873 Epoch 70, Training loss 0.21108372443041223\n",
            "2021-01-18 06:48:51.923412 Epoch 80, Training loss 0.19629904437976278\n",
            "2021-01-18 06:49:25.295719 Epoch 90, Training loss 0.1811957617948769\n",
            "2021-01-18 06:49:58.331065 Epoch 100, Training loss 0.16627288676181418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMQw6Zg-UgA-"
      },
      "source": [
        "### Measuring accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1rIffg0UhAZ"
      },
      "source": [
        "In order to have a measure that is more interpretable than the loss, we can take a look at our accuracies on the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdOBbYsFT23v"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\r\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3qqlTa08Waj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a75c744-64eb-47d8-8df7-e1aab83f081b"
      },
      "source": [
        "def validate(model, train_loader, val_loader):\r\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    # We do not want gradients here, as we will not want to update the parameters\r\n",
        "    with torch.no_grad():\r\n",
        "      for imgs, labels in loader:\r\n",
        "        outputs = model(imgs)\r\n",
        "        # Gives us the index of the highest value as output\r\n",
        "        _, predicted = torch.max(outputs, dim=1)\r\n",
        "        # Counts the number of examples, so total is increased by the batch size\r\n",
        "        total += labels.shape[0]\r\n",
        "        # Comparing the predicted class that had the maximum probability and the ground-truth labels, we first get a Boolean array. Taking the\r\n",
        "        # sum gives the number of items in the batch where the prediction and ground truth agree.\r\n",
        "        correct += int((predicted == labels).sum())\r\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name, correct / total))\r\n",
        "\r\n",
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.94\n",
            "Accuracy val: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGMHP80O-hMs"
      },
      "source": [
        "This is quite a lot better than the fully connected model, which achieved only 79% accuracy. We about halved the number of errors on the validation set. Also, we used far fewer parameters. **This is telling us that the model does a better job of generalizing its task of recognizing the subject of images from a new sample, through locality and translation invariance.** We could now let it run for more epochs and see what performance we could squeeze out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cEaZVn-xgW"
      },
      "source": [
        "### Saving and loading our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7z7yZ0B-yYi"
      },
      "source": [
        "Since we’re satisfied with our model so far, it would be nice to actually save it, right?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKsK2Kot-Mqx"
      },
      "source": [
        "torch.save(model.state_dict(), data_path + \"birds_vs_airplanes.pt\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1zlYIzM_SeB"
      },
      "source": [
        "The birds_vs_airplanes.pt file now contains all the parameters of model: that is, weights and biases for the two convolution modules and the two linear modules. So, no structure—just the weights. **This means when we deploy the model in production for our friend, we’ll need to keep the model class handy, create an instance, and then load the parameters back into it**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mZxIood_o9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e803f749-decd-42f2-d139-e299cf83c010"
      },
      "source": [
        "# We will have to make sure we don’t change the definition of Net between saving and later loading the model state.\r\n",
        "loaded_model = Net()\r\n",
        "loaded_model.load_state_dict(torch.load(data_path + \"birds_vs_airplanes.pt\"))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NV-DKuyA7qv"
      },
      "source": [
        "### Training on the GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n68P_QU0A70_"
      },
      "source": [
        "It is considered good style to move things to the GPU if one is available. A good pattern is to set the a variable device depending on torch.cuda.is_available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rbORur8_1ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1199fd8-e928-4c8d-9640-6e7b49817b0d"
      },
      "source": [
        "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\r\n",
        "print(f\"Training on device {device}.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCMczbvCBbrS"
      },
      "source": [
        "Then we can amend the training loop by moving the tensors we get from the data\r\n",
        "loader to the GPU by using the Tensor.to method. Note that the code is exactly like our first version at the beginning of this section except for the two lines moving the inputs to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6YqVZNZBSTb"
      },
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\r\n",
        "  # loop over the epochs, numbered from 1 to n_epochs rather than starting at 0\r\n",
        "  for epoch in range(1, n_epochs + 1):\r\n",
        "    loss_train = 0.0\r\n",
        "    # Loops over dataset in the batches the data loader creates for us\r\n",
        "    for imgs, labels in train_loader:\r\n",
        "      # These two lines that move imgs and labels to the GPU device\r\n",
        "      imgs = imgs.to(device=device)\r\n",
        "      labels = labels.to(device=device)\r\n",
        "      # Feeds a batch through our model\r\n",
        "      outputs = model(imgs)\r\n",
        "      # and computes the loss we wish to minimize\r\n",
        "      loss = loss_fn(outputs, labels)\r\n",
        "      # After getting rid of the gradients from the last round\r\n",
        "      optimizer.zero_grad()\r\n",
        "      # performs the backward step. That is, we compute the gradients of all parameters we want the network to learn.\r\n",
        "      loss.backward()\r\n",
        "      # Updates the model\r\n",
        "      optimizer.step()\r\n",
        "      # Sums the losses\r\n",
        "      loss_train += loss.item()\r\n",
        "    if epoch == 1 or epoch % 10 == 0:\r\n",
        "      # Divides by the length of the training data loader to get the average loss per batch.\r\n",
        "      print(\"{} Epoch {}, Training loss {}\".format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sxJmqMNCUgo"
      },
      "source": [
        "The same amendment must be made to the validate function. We can then instantiate our model, move it to device, and run it as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENJm9SUBCN94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a482eef-6047-490f-fa36-d67bf8540e77"
      },
      "source": [
        "# The DataLoader batches up the examples of our cifar2 dataset. \r\n",
        "# Shuffling randomizes the order of the examples from the dataset.\r\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\r\n",
        "\r\n",
        "# Moves our model (all parameters) to the GPU. If you forget to move either the model or the inputs to the GPU, \r\n",
        "# you will get errors about tensors not being on the same device, because the PyTorch operators do not support mixing GPU and CPU inputs.\r\n",
        "model = Net().to(device=device)\r\n",
        "\r\n",
        "# the stochastic gradient descent optimizer we have been working with\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\r\n",
        "# and the cross entropy loss\r\n",
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "training_loop(n_epochs=100, optimizer=optimizer, model=model, loss_fn=loss_fn, train_loader=train_loader)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-18 06:50:11.825812 Epoch 1, Training loss 0.5637948120095927\n",
            "2021-01-18 06:50:15.234206 Epoch 10, Training loss 0.33427952210994283\n",
            "2021-01-18 06:50:19.000042 Epoch 20, Training loss 0.29963565527633496\n",
            "2021-01-18 06:50:22.674725 Epoch 30, Training loss 0.2760529989839359\n",
            "2021-01-18 06:50:26.467777 Epoch 40, Training loss 0.25198076560998417\n",
            "2021-01-18 06:50:30.197634 Epoch 50, Training loss 0.2354876129965114\n",
            "2021-01-18 06:50:33.860346 Epoch 60, Training loss 0.22041681185839282\n",
            "2021-01-18 06:50:37.606905 Epoch 70, Training loss 0.20427968025587168\n",
            "2021-01-18 06:50:41.547947 Epoch 80, Training loss 0.1874170883256159\n",
            "2021-01-18 06:50:45.374443 Epoch 90, Training loss 0.17569316036192476\n",
            "2021-01-18 06:50:49.334110 Epoch 100, Training loss 0.16044840719669487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwS9mSjkC-HY"
      },
      "source": [
        "Even for our small network here, we do see a sizable increase in speed. The advantage of computing on GPUs is more visible for larger models.\r\n",
        "\r\n",
        "It is a bit more concise to instruct PyTorch to override the device\r\n",
        "information when loading weights. This is done by passing the map_location keyword argument to torch.load:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdG6o5SCCzua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40780af-0684-4d87-d09e-5c3401e6dfd9"
      },
      "source": [
        "# We will have to make sure we don’t change the definition of Net between saving and later loading the model state.\r\n",
        "loaded_model = Net().to(device=device)\r\n",
        "loaded_model.load_state_dict(torch.load(data_path + \"birds_vs_airplanes.pt\", map_location=device))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0BE6ij1EGF2"
      },
      "source": [
        "## Model design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le9Tu1RfEG9v"
      },
      "source": [
        "Plus images may not be our sole focus in the real world, where we have tabular\r\n",
        "data, sequences, and text. The promise of neural networks is sufficient flexibility to solve problems on all these kinds of data given the proper architecture (that is, the interconnection of layers or modules) and the proper loss function.\r\n",
        "\r\n",
        "PyTorch ships with a very comprehensive collection of modules and loss functions\r\n",
        "to implement state-of-the-art architectures ranging from feed-forward components to long short-term memory (LSTM) modules and transformer networks (two very popular architectures for sequential data). Several models are available through PyTorch Hub or as part of torchvision and other vertical community efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVPbiHnXQMzV"
      },
      "source": [
        "### Adding memory capacity: Width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw5aC7OrQOOS"
      },
      "source": [
        "Given our feed-forward architecture, there are a couple of dimensions we’d likely want to explore before getting into further complications. The first dimension is the width of the network: the number of neurons per layer, or channels per convolution.\r\n",
        "\r\n",
        "\r\n",
        "We can make a model wider very easily in PyTorch. We just specify a larger number of output channels in the first convolution and increase the subsequent layers accordingly, taking care to change the forward function to reflect the fact that we’ll now have a longer vector once we switch to fully connected layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abXIy25fD9Oo"
      },
      "source": [
        "class WideNetwork(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\r\n",
        "    self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\r\n",
        "    self.fc1 = nn.Linear(16 * 8 * 8, 32)\r\n",
        "    self.fc2 = nn.Linear(32, 2)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\r\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\r\n",
        "    out = out.view(-1, 16 * 8 * 8)\r\n",
        "    out = torch.tanh(self.fc1(out))\r\n",
        "    out = self.fc2(out)\r\n",
        "\r\n",
        "    return out"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4UUpD-sQ3Zl"
      },
      "source": [
        "If we want to avoid hardcoding numbers in the definition of the model, we can easily pass a parameter to init and parameterize the width, taking care to also parameterize the call to view in the forward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDgwgr0mQ7Xp"
      },
      "source": [
        "class WideNetwork(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, n_channel=32):\r\n",
        "    super().__init__()\r\n",
        "    self.n_channel = n_channel\r\n",
        "    self.conv1 = nn.Conv2d(3, n_channel, kernel_size=3, padding=1)\r\n",
        "    self.conv2 = nn.Conv2d(n_channel, n_channel // 2, kernel_size=3, padding=1)\r\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_channel // 2, 32)\r\n",
        "    self.fc2 = nn.Linear(32, 2)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\r\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\r\n",
        "    out = out.view(-1, 8 * 8 * self.n_channel // 2)\r\n",
        "    out = torch.tanh(self.fc1(out))\r\n",
        "    out = self.fc2(out)\r\n",
        "\r\n",
        "    return out"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_cp6ipzSrQa",
        "outputId": "840baf64-a377-46b0-e67a-e781c3ac541b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The DataLoader batches up the examples of our cifar2 dataset. \r\n",
        "# Shuffling randomizes the order of the examples from the dataset.\r\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\r\n",
        "\r\n",
        "# Moves our model (all parameters) to the GPU. If you forget to move either the model or the inputs to the GPU, \r\n",
        "# you will get errors about tensors not being on the same device, because the PyTorch operators do not support mixing GPU and CPU inputs.\r\n",
        "model = WideNetwork().to(device=device)\r\n",
        "\r\n",
        "# the stochastic gradient descent optimizer we have been working with\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\r\n",
        "# and the cross entropy loss\r\n",
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "training_loop(n_epochs=100, optimizer=optimizer, model=model, loss_fn=loss_fn, train_loader=train_loader)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-18 06:59:43.410872 Epoch 1, Training loss 0.5373456705907348\n",
            "2021-01-18 06:59:47.234313 Epoch 10, Training loss 0.3100430370326255\n",
            "2021-01-18 06:59:51.336249 Epoch 20, Training loss 0.26951973007363117\n",
            "2021-01-18 06:59:55.561712 Epoch 30, Training loss 0.23577301242169302\n",
            "2021-01-18 06:59:59.836520 Epoch 40, Training loss 0.20491675875938622\n",
            "2021-01-18 07:00:04.062203 Epoch 50, Training loss 0.17928042566510521\n",
            "2021-01-18 07:00:08.318892 Epoch 60, Training loss 0.15883874463712333\n",
            "2021-01-18 07:00:12.615775 Epoch 70, Training loss 0.13749011921559928\n",
            "2021-01-18 07:00:16.843254 Epoch 80, Training loss 0.1192706318892491\n",
            "2021-01-18 07:00:20.938492 Epoch 90, Training loss 0.09973475405839598\n",
            "2021-01-18 07:00:25.133886 Epoch 100, Training loss 0.08441405498962494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwY5Q96URvYm"
      },
      "source": [
        "The numbers specifying channels and features for each layer are directly related to the number of parameters in a model; all other things being equal, they increase the capacity of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JykDkkenRseF",
        "outputId": "0b4da576-a21e-48cf-8da7-95d0cb357780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38386"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQDK20JiUpfN"
      },
      "source": [
        "The greater the capacity, the more variability in the inputs the model will be able to manage; but at the same time, the more likely overfitting will be, since the model can use a greater number of parameters to memorize unessential aspects of the input. We already went into ways to combat overfitting, the best being increasing the sample size or, in the absence of new data, augmenting existing data through artificial modifications of the same data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMODb9deU1-E"
      },
      "source": [
        "### Helping our model to converge and generalize: Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bINQvrRU3ym"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmeedt2MR6Qw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}