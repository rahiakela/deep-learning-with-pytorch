{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-convolutions-in-action.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLXlEdi9VvevYCr9u4t2WT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4905f17a8944a3394c9cf381d9a2ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_710d2ca997554bcf85dc001dae0819d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a5cfddac0a374594a07ffce4488f428e",
              "IPY_MODEL_e9b627640d6d4127a61539bfedb0bce0"
            ]
          }
        },
        "710d2ca997554bcf85dc001dae0819d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5cfddac0a374594a07ffce4488f428e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a805ee10a6a40858353f6b43f4df4ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b7c85c22cf24d2c949dbbbbd38d91e1"
          }
        },
        "e9b627640d6d4127a61539bfedb0bce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_042d8cce22524a259699c62531ed8639",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 52472991.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7e7330665454e3d9dba607a73eaf653"
          }
        },
        "9a805ee10a6a40858353f6b43f4df4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b7c85c22cf24d2c949dbbbbd38d91e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "042d8cce22524a259699c62531ed8639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7e7330665454e3d9dba607a73eaf653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-with-pytorch/blob/master/8-using-convolutions-to-generalize/1_convolutions_in_action.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD-pGnbRyH6s"
      },
      "source": [
        "# Using convolutions to generalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QS-_bhIydI7"
      },
      "source": [
        "Due to the fully connected setup needed to detect the various possible translations of the bird or airplane in the image, we have both too many parameters (making it easier for the model to memorize the training set) and no position independence (making it harder to generalize). \r\n",
        "\r\n",
        "As we know, we could augment our training data by using a wide variety of recropped images to try to force generalization, but that won’t address the issue of having too many parameters.\r\n",
        "**There is a better way! It consists of replacing the dense, fully connected affine transformation in our neural network unit with a different linear operation: convolution.**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRKStm2QzDAv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fRJTzpxzEj7"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import collections\r\n",
        "import datetime\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "torch.set_printoptions(edgeitems=2)\r\n",
        "torch.manual_seed(123)\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "c4905f17a8944a3394c9cf381d9a2ed3",
            "710d2ca997554bcf85dc001dae0819d6",
            "a5cfddac0a374594a07ffce4488f428e",
            "e9b627640d6d4127a61539bfedb0bce0",
            "9a805ee10a6a40858353f6b43f4df4ab",
            "6b7c85c22cf24d2c949dbbbbd38d91e1",
            "042d8cce22524a259699c62531ed8639",
            "d7e7330665454e3d9dba607a73eaf653"
          ]
        },
        "id": "-T4StfdbzKPO",
        "outputId": "e915f1af-c9cf-4a8e-94bd-c15691bf77d4"
      },
      "source": [
        "data_path = '../data-unversioned/p1ch6/'\r\n",
        "cifar10 = datasets.CIFAR10(\r\n",
        "    data_path, train=True, download=True,\r\n",
        "    transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\r\n",
        "                             (0.2470, 0.2435, 0.2616))\r\n",
        "    ]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4905f17a8944a3394c9cf381d9a2ed3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zOHe4ySzqB1",
        "outputId": "de661eb1-1f67-4acb-bcbf-612978cbcbf8"
      },
      "source": [
        "cifar10_val = datasets.CIFAR10(\r\n",
        "    data_path, train=False, download=True,\r\n",
        "    transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\r\n",
        "                             (0.2470, 0.2435, 0.2616))\r\n",
        "    ]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASLMMCKzztlJ"
      },
      "source": [
        "label_map = {0: 0, 2: 1}\r\n",
        "class_names = ['airplane', 'bird']\r\n",
        "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\r\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfKQQ6X00TyU"
      },
      "source": [
        "## The case for convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-79ivp0UzG"
      },
      "source": [
        "We know that taking a 1D view of our input image and multiplying it by an\r\n",
        "n_output_features × n_input_features weight matrix, as is done in nn.Linear,\r\n",
        "means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn5xF0Uhz2es"
      },
      "source": [
        "connected_model = nn.Sequential(\r\n",
        "            nn.Linear(3072, 1024),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(1024, 512),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(512, 128),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(128, 2))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1F_nv30kBn",
        "outputId": "cfd2f3d7-3b73-4429-dd73-01b621b84aeb"
      },
      "source": [
        "numel_list = [p.numel()\r\n",
        "              for p in connected_model.parameters()\r\n",
        "              if p.requires_grad == True]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfUq-5Wg1Pyk"
      },
      "source": [
        "first_model = nn.Sequential(\r\n",
        "                nn.Linear(3072, 512),\r\n",
        "                nn.Tanh(),\r\n",
        "                nn.Linear(512, 2),\r\n",
        "                nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ejlQD701TAj",
        "outputId": "e11215c4-c498-494f-9a62-a42b670c9b46"
      },
      "source": [
        "numel_list = [p.numel() for p in first_model.parameters()]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1574402, [1572864, 512, 1024, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWhqZSQG1XOY",
        "outputId": "66d72fd3-870b-4e9d-9dc9-02cea2e3af0e"
      },
      "source": [
        "linear = nn.Linear(3072, 1024)\r\n",
        "\r\n",
        "linear.weight.shape, linear.bias.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 3072]), torch.Size([1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSstZM7603UJ"
      },
      "source": [
        "In order to translate this intuition into mathematical form, we could compute the weighted sum of a pixel with its immediate neighbors, rather than with all other pixels in the image. This would be equivalent to building weight matrices, one per output feature and output pixel location, in which all weights beyond a certain distance from a center pixel are zero. This will still be a weighted sum: that is, a linear operation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQENTdNY1a74"
      },
      "source": [
        "## What convolutions do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvVc6eWe1eZB"
      },
      "source": [
        "We identified one more desired property earlier: we would like these localized patterns to have an effect on the output regardless of their location in the image: that is, to be translation invariant.\r\n",
        "\r\n",
        "**Fortunately, there is a readily available, local, translation-invariant linear operation on the image: a convolution.**\r\n",
        "\r\n",
        "**Convolution, or more precisely, discrete convolution is defined for a 2D image as the scalar product of a weight matrix, the kernel, with every neighborhood in the input.**\r\n",
        "\r\n",
        "Consider a 3 × 3 kernel (in deep learning, we typically use small kernels; we’ll see why later on) as a 2D tensor.\r\n",
        "\r\n",
        "```python\r\n",
        "weight = torch.tensor([\r\n",
        "  [w00, w01, w02],\r\n",
        "  [w10, w11, w12],\r\n",
        "  [w20, w21, w22]\r\n",
        "])\r\n",
        "```\r\n",
        "\r\n",
        "and a 1-channel, MxN image:\r\n",
        "\r\n",
        "```python\r\n",
        "image = torch.tensor([\r\n",
        "  [i00, i01, i02, i03, ..., i0N],\r\n",
        "  [i10, i11, i12, i13, ..., i1N],\r\n",
        "  [i20, i21, i22, i23, ..., i2N],\r\n",
        "  [i30, i31, i32, i33, ..., i3N],\r\n",
        "  ...\r\n",
        "  [iM0, iM1m iM2, iM3, ..., iMN]\r\n",
        "])\r\n",
        "```\r\n",
        "\r\n",
        "We can compute an element of the output image (without bias) as follows:\r\n",
        "\r\n",
        "```python\r\n",
        "o11 = i11 * w00 + i12 * w01 + i22 * w02 +\r\n",
        "      i21 * w10 + i22 * w11 + i23 * w12 +\r\n",
        "      i31 * w20 + i32 * w21 + i33 * w22\r\n",
        "```\r\n",
        "\r\n",
        "That is, we “translate” the kernel on the i11 location of the input image, and we multiply each weight by the value of the input image at the corresponding location. Thus, the output image is created by translating the kernel on all input locations and performing the weighted sum. For a multichannel image, like our RGB image, the weight matrix would be a 3 × 3 × 3 matrix: one set of weights for every channel, contributing together to the output values.\r\n",
        "\r\n",
        "Note that, just like the elements in the weight matrix of nn.Linear, the weights in the kernel are not known in advance, but they are initialized randomly and updated through backpropagation. Note also that the same kernel, and thus each weight in the kernel, is reused across the whole image. Thinking back to autograd, this means the use of each weight has a history spanning the entire image. Thus, the derivative of the loss with respect to a convolution weight includes contributions from the entire image.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/locality-and-translation-invariance.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "It’s now possible to see the connection to what we were stating earlier:**a convolution is equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.**\r\n",
        "\r\n",
        "Summarizing, by switching to convolutions, we get:\r\n",
        "\r\n",
        "- **Local operations on neighborhoods**\r\n",
        "- **Local operations on neighborhoods**\r\n",
        "- **Models with a lot fewer parameters**\r\n",
        "\r\n",
        "The key insight underlying the third point is that, with a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case in our fully connected model, but rather on the size of the convolution kernel (3 × 3, 5 × 5, and so on) and on how many convolution filters (or output channels) we decide to use in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmHH9aRu6gW1"
      },
      "source": [
        "## Convolutions in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSjCHkTU6jMf"
      },
      "source": [
        "Let’s see some PyTorch in action on our birds versus airplanes challenge. The torch.nn module provides convolutions for 1, 2, and 3 dimensions: nn.Conv1d for time series, nn.Conv2d for images, and nn.Conv3d for volumes or videos.\r\n",
        "\r\n",
        "At a minimum, the arguments we provide to nn.Conv2d are the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel), the number of output features, and the size of the kernel.\r\n",
        "\r\n",
        "The more channels in the output image, the more the capacity of the network. We need the channels to be able to detect many different types of features.\r\n",
        "\r\n",
        "It is very common to have kernel sizes that are the same in all directions, so\r\n",
        "PyTorch has a shortcut for this: whenever kernel_size=3 is specified for a 2D convolution, it means 3 × 3 (provided as a tuple (3, 3) in Python). For a 3D convolution, it means 3 × 3 × 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HsvvkNm0m2S",
        "outputId": "bd88f84b-7618-44d6-b7bb-51c1b707a911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv = nn.Conv2d(3, 16, kernel_size=3)\r\n",
        "conv"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzIhOZswVDjd"
      },
      "source": [
        "What do we expect to be the shape of the weight tensor? \r\n",
        "\r\n",
        "The kernel is of size `3 × 3`, so we want the weight to consist of `3 × 3` parts. For a single output pixel value, our kernel would consider, say, `in_ch = 3` input channels, so the weight component for a single output pixel value (and by translation the invariance for the entire output channel) is of shape `in_ch × 3 × 3`. Finally, we have as many of those as we have output channels, here `out_ch = 16`, so the complete weight tensor is `out_ch × in_ch × 3 × 3`, in our case `16 × 3 × 3 × 3`. The bias will have size 16.\r\n",
        "\r\n",
        "Let’s verify our assumptions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M61bQt5pUVL9",
        "outputId": "26bd09b0-1a51-426a-e3be-c39718afe570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv.weight.shape, conv.bias.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_wQGVFvVoLK"
      },
      "source": [
        "We can see how convolutions are a convenient choice for learning from images. We\r\n",
        "have smaller models looking for local patterns whose weights are optimized across the entire image.\r\n",
        "\r\n",
        "A 2D convolution pass produces a 2D image as output, whose pixels are a weighted\r\n",
        "sum over neighborhoods of the input image.\r\n",
        "\r\n",
        "As usual, we need to add the zeroth batch dimension with\r\n",
        "unsqueeze if we want to call the conv module with one input image, since nn.Conv2d expects a B × C × H × W shaped tensor as input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_djc2dVVc0i",
        "outputId": "f62634bd-5bbf-437a-fc47-897006569b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img, _ = cifar2[0]\r\n",
        "output = conv(img.unsqueeze(0))\r\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceth4g6tWUqW"
      },
      "source": [
        "We’re curious, so we can display the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJEcnoEWPnI",
        "outputId": "bd9d41aa-42e6-4177-f737-0f0469e0c9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "plt.imshow(output[0, 0].detach(), cmap=\"gray\")\r\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9evxvQbWifv",
        "outputId": "a93c4db5-625e-45c2-d16b-86f0d3a489ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "plt.figure(figsize=(10, 4.8)) \r\n",
        "ax1 = plt.subplot(1, 2, 1)   \r\n",
        "plt.title('output')   \r\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\r\n",
        "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1) \r\n",
        "plt.imshow(img.mean(0), cmap='gray')  \r\n",
        "plt.title('input')  \r\n",
        " \r\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x345.6 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WDcxQIAXAN3"
      },
      "source": [
        "Wait a minute. Let’s take a look a the size of output: it’s `torch.Size([1, 16, 30, 30])`. Huh; we lost a few pixels in the process. How did that happen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4aamEQvXGzK"
      },
      "source": [
        "## Padding the boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPStthf9XH13"
      },
      "source": [
        "By default, PyTorch will slide the convolution kernel within the input picture, getting `width - kernel_width + 1` horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, 3//2 = 1) smaller on each side. This explains why we’re missing two pixels in each dimension.\r\n",
        "\r\n",
        "However, PyTorch gives us the possibility of padding the image by creating ghost pixels around the border that have value zero as far as the convolution is concerned.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/zero-padding.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "In our case, specifying padding=1 when kernel_size=3 means i00 has an extra set\r\n",
        "of neighbors above it and to its left, so that an output of the convolution can be computed even in the corner of our original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3VzAsO4Wyuk",
        "outputId": "25653aa3-0a27-436c-c32c-91e131584174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now with padding\r\n",
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\r\n",
        "output = conv(img.unsqueeze(0))\r\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUxNnVYc33A"
      },
      "source": [
        "There are two main reasons to pad convolutions. \r\n",
        "\r\n",
        "First, doing so helps us separate the matters of convolution and changing image sizes, so we have one less thing to remember. \r\n",
        "\r\n",
        "And second, when we have more elaborate structures such as skip connections or the U-Nets we’ll cover in part 2, we want the tensors before and after a few convolutions to be of compatible size so that we can add them or take differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8GFtKsIdDM4"
      },
      "source": [
        "## Detecting features with convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-V1r3JtdGQK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR6D0sRScSuZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}