{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-convolutions-in-action.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5s01au9YApbvCiWjgxsWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a3d2b7fce2e49c598d89c82295bb4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_19ee485477bc44a09f5c27080a566ae2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4b77ebde90dd45389edb49613dcc7b2f",
              "IPY_MODEL_3057680f6d2241d788a22804dd718ac9"
            ]
          }
        },
        "19ee485477bc44a09f5c27080a566ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b77ebde90dd45389edb49613dcc7b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_16148387c7214d3ba42ef4a27d1ff91a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0a776bdcf6f4989b206a9b08610a67f"
          }
        },
        "3057680f6d2241d788a22804dd718ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6339fff6dfd64559921dab8b39c054a4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 77421622.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_14807282d62c4edba3efd5d250d2de96"
          }
        },
        "16148387c7214d3ba42ef4a27d1ff91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0a776bdcf6f4989b206a9b08610a67f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6339fff6dfd64559921dab8b39c054a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "14807282d62c4edba3efd5d250d2de96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-with-pytorch/blob/master/8-using-convolutions-to-generalize/1_convolutions_in_action.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD-pGnbRyH6s"
      },
      "source": [
        "# Using convolutions to generalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QS-_bhIydI7"
      },
      "source": [
        "Due to the fully connected setup needed to detect the various possible translations of the bird or airplane in the image, we have both too many parameters (making it easier for the model to memorize the training set) and no position independence (making it harder to generalize). \r\n",
        "\r\n",
        "As we know, we could augment our training data by using a wide variety of recropped images to try to force generalization, but that won’t address the issue of having too many parameters.\r\n",
        "**There is a better way! It consists of replacing the dense, fully connected affine transformation in our neural network unit with a different linear operation: convolution.**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRKStm2QzDAv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fRJTzpxzEj7"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import collections\r\n",
        "import datetime\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "torch.set_printoptions(edgeitems=2)\r\n",
        "torch.manual_seed(123)\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4a3d2b7fce2e49c598d89c82295bb4d4",
            "19ee485477bc44a09f5c27080a566ae2",
            "4b77ebde90dd45389edb49613dcc7b2f",
            "3057680f6d2241d788a22804dd718ac9",
            "16148387c7214d3ba42ef4a27d1ff91a",
            "c0a776bdcf6f4989b206a9b08610a67f",
            "6339fff6dfd64559921dab8b39c054a4",
            "14807282d62c4edba3efd5d250d2de96"
          ]
        },
        "id": "-T4StfdbzKPO",
        "outputId": "89cfbaa6-7fa2-406a-e02e-fbabebc06ae0"
      },
      "source": [
        "data_path = '../data-unversioned/p1ch6/'\r\n",
        "cifar10 = datasets.CIFAR10(\r\n",
        "    data_path, train=True, download=True,\r\n",
        "    transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\r\n",
        "                             (0.2470, 0.2435, 0.2616))\r\n",
        "    ]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a3d2b7fce2e49c598d89c82295bb4d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zOHe4ySzqB1",
        "outputId": "1ae078e4-e4d0-40a1-c21c-48fe3e2bad4f"
      },
      "source": [
        "cifar10_val = datasets.CIFAR10(\r\n",
        "    data_path, train=False, download=True,\r\n",
        "    transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\r\n",
        "                             (0.2470, 0.2435, 0.2616))\r\n",
        "    ]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASLMMCKzztlJ"
      },
      "source": [
        "label_map = {0: 0, 2: 1}\r\n",
        "class_names = ['airplane', 'bird']\r\n",
        "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\r\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfKQQ6X00TyU"
      },
      "source": [
        "## The case for convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-79ivp0UzG"
      },
      "source": [
        "We know that taking a 1D view of our input image and multiplying it by an\r\n",
        "n_output_features × n_input_features weight matrix, as is done in nn.Linear,\r\n",
        "means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn5xF0Uhz2es"
      },
      "source": [
        "connected_model = nn.Sequential(\r\n",
        "            nn.Linear(3072, 1024),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(1024, 512),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(512, 128),\r\n",
        "            nn.Tanh(),\r\n",
        "            nn.Linear(128, 2))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1F_nv30kBn",
        "outputId": "516515da-88c8-48bb-d8b8-51f01fcfa315"
      },
      "source": [
        "numel_list = [p.numel()\r\n",
        "              for p in connected_model.parameters()\r\n",
        "              if p.requires_grad == True]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfUq-5Wg1Pyk"
      },
      "source": [
        "first_model = nn.Sequential(\r\n",
        "                nn.Linear(3072, 512),\r\n",
        "                nn.Tanh(),\r\n",
        "                nn.Linear(512, 2),\r\n",
        "                nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ejlQD701TAj",
        "outputId": "19473d18-6fe1-4454-9a41-db8a7d18dd2c"
      },
      "source": [
        "numel_list = [p.numel() for p in first_model.parameters()]\r\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1574402, [1572864, 512, 1024, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWhqZSQG1XOY",
        "outputId": "f3b198b4-54a2-4ec9-eb6c-f66b9e509cce"
      },
      "source": [
        "linear = nn.Linear(3072, 1024)\r\n",
        "\r\n",
        "linear.weight.shape, linear.bias.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 3072]), torch.Size([1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSstZM7603UJ"
      },
      "source": [
        "In order to translate this intuition into mathematical form, we could compute the weighted sum of a pixel with its immediate neighbors, rather than with all other pixels in the image. This would be equivalent to building weight matrices, one per output feature and output pixel location, in which all weights beyond a certain distance from a center pixel are zero. This will still be a weighted sum: that is, a linear operation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQENTdNY1a74"
      },
      "source": [
        "## What convolutions do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvVc6eWe1eZB"
      },
      "source": [
        "We identified one more desired property earlier: we would like these localized patterns to have an effect on the output regardless of their location in the image: that is, to be translation invariant.\r\n",
        "\r\n",
        "**Fortunately, there is a readily available, local, translation-invariant linear operation on the image: a convolution.**\r\n",
        "\r\n",
        "**Convolution, or more precisely, discrete convolution is defined for a 2D image as the scalar product of a weight matrix, the kernel, with every neighborhood in the input.**\r\n",
        "\r\n",
        "Consider a 3 × 3 kernel (in deep learning, we typically use small kernels; we’ll see why later on) as a 2D tensor.\r\n",
        "\r\n",
        "```python\r\n",
        "weight = torch.tensor([\r\n",
        "  [w00, w01, w02],\r\n",
        "  [w10, w11, w12],\r\n",
        "  [w20, w21, w22]\r\n",
        "])\r\n",
        "```\r\n",
        "\r\n",
        "and a 1-channel, MxN image:\r\n",
        "\r\n",
        "```python\r\n",
        "image = torch.tensor([\r\n",
        "  [i00, i01, i02, i03, ..., i0N],\r\n",
        "  [i10, i11, i12, i13, ..., i1N],\r\n",
        "  [i20, i21, i22, i23, ..., i2N],\r\n",
        "  [i30, i31, i32, i33, ..., i3N],\r\n",
        "  ...\r\n",
        "  [iM0, iM1m iM2, iM3, ..., iMN]\r\n",
        "])\r\n",
        "```\r\n",
        "\r\n",
        "We can compute an element of the output image (without bias) as follows:\r\n",
        "\r\n",
        "```python\r\n",
        "o11 = i11 * w00 + i12 * w01 + i22 * w02 +\r\n",
        "      i21 * w10 + i22 * w11 + i23 * w12 +\r\n",
        "      i31 * w20 + i32 * w21 + i33 * w22\r\n",
        "```\r\n",
        "\r\n",
        "That is, we “translate” the kernel on the i11 location of the input image, and we multiply each weight by the value of the input image at the corresponding location. Thus, the output image is created by translating the kernel on all input locations and performing the weighted sum. For a multichannel image, like our RGB image, the weight matrix would be a 3 × 3 × 3 matrix: one set of weights for every channel, contributing together to the output values.\r\n",
        "\r\n",
        "Note that, just like the elements in the weight matrix of nn.Linear, the weights in the kernel are not known in advance, but they are initialized randomly and updated through backpropagation. Note also that the same kernel, and thus each weight in the kernel, is reused across the whole image. Thinking back to autograd, this means the use of each weight has a history spanning the entire image. Thus, the derivative of the loss with respect to a convolution weight includes contributions from the entire image.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-pytorch/locality-and-translation-invariance.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "It’s now possible to see the connection to what we were stating earlier:**a convolution is equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.**\r\n",
        "\r\n",
        "Summarizing, by switching to convolutions, we get:\r\n",
        "\r\n",
        "- **Local operations on neighborhoods**\r\n",
        "- **Local operations on neighborhoods**\r\n",
        "- **Models with a lot fewer parameters**\r\n",
        "\r\n",
        "The key insight underlying the third point is that, with a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case in our fully connected model, but rather on the size of the convolution kernel (3 × 3, 5 × 5, and so on) and on how many convolution filters (or output channels) we decide to use in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmHH9aRu6gW1"
      },
      "source": [
        "## Convolutions in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSjCHkTU6jMf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HsvvkNm0m2S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}